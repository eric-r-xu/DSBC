{
 "metadata": {
  "name": "",
  "signature": "sha256:3bc47a2328651bdc7869578cb14ac6eb06e7edc3b839e4b89dbb74b59134bf78"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sentence tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pip install nltk\n",
      "from nltk.tokenize import sent_tokenize\n",
      "\n",
      "text = \"Hello. How are you, dear sir? Are you well? Here: drink this! It will make you feel better.\"\n",
      "\n",
      "sentences = sent_tokenize(text)\n",
      "sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Word tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import TreebankWordTokenizer\n",
      "tokenizer = TreebankWordTokenizer()\n",
      "tokenizer.tokenize(sentences[4])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize\n",
      "words = word_tokenize(sentences[4])\n",
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### When you say \"word\"..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(\"Who's going to that thing today?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import wordpunct_tokenize\n",
      "wordpunct_tokenize(\"Who's going to that thing today?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Demo of different tokenizers: http://text-processing.com/demo/tokenize/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Part of speech tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import pos_tag\n",
      "words = word_tokenize(\"Who's going to that thing today?\")\n",
      "pos_tag(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "WP: wh-pronoun (\"who\", \"what\")  \n",
      "VBZ: verb, 3rd person sing. present (\"takes\")  \n",
      "VBG: verb, gerund/present participle (\"taking\")  \n",
      "TO: to (\"to go\", \"to him\")   \n",
      "DT: determiner (\"the\", \"this\")  \n",
      "NN: noun, singular or mass (\"door\")  \n",
      ".: Punctuation (\".\", \"?\")  \n",
      "\n",
      "All tags: http://www.monlp.com/2011/11/08/part-of-speech-tags/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part of speech allows you to focus on different parts of language.  \n",
      "You may want to find keywords only among verbs, for example.  \n",
      "Or when you are classifying, you may want to use nouns and adjectives as features, because they carry the most information about the subject (compared to who tags like WP, or TO, etc.) Part of speech can allow you to do higher resolution text analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Chunking\n",
      "Extracting phrases"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "#nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.chunk import ne_chunk\n",
      "words = word_tokenize(\"I'm Irmak Sirer and I'm here to teach you NLTK today.\")\n",
      "tags = pos_tag(words)\n",
      "tree = ne_chunk(tags)\n",
      "print tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Included text corpora\n",
      "movie_reviews: Imdb reviews characterized as pos & neg  \n",
      "treebank: tagged and parsed Wall Street Journal text  \n",
      "treebank_chunk: tagged and chunked WSJ text  \n",
      "brown: tagged & categorized English text (news, fiction, etc)  \n",
      "60 others  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download()\n",
      "from nltk.corpus import treebank_chunk\n",
      "treebank_chunk.tagged_sents()[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treebank_chunk.chunked_sents()[0].draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TextBlob"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pip install textblob\n",
      "from textblob import TextBlob\n",
      "\n",
      "GATSBY_TEXT = \"\"\"In my younger and more vulnerable years my father gave me some advice that \n",
      "I've been turning over in my mind ever since. \"Whenever you feel like criticizing any one,\" \n",
      "he told me, \"blah blah, blah blah blah.\"\"\"\"\"\n",
      "\n",
      "gatsby = TextBlob(GATSBY_TEXT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"Oh my god I love this bootcamp, it's so awesome.\").sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"Cupcakes are the worst.\").sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"The color of this car is blue\").sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print TextBlob(\"I hate cupcakes.\").sentiment.polarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print TextBlob(\"I hate cupcakes.\").sentiment.subjectivity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.sentences[0].words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = nltk.stem.porter.PorterStemmer()\n",
      "for word in TextBlob(\"I was going to go to many places\").words:\n",
      "    print stemmer.stem(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see different nltk stemmers in effect:\n",
      "http://text-processing.com/demo/stem/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word, count in gatsby.word_counts.items():\n",
      "    print \"%15s %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_count(item):\n",
      "    return item[1]\n",
      "for word, count in sorted(gatsby.word_counts.items(), key=get_count, reverse=True):\n",
      "    print \"%15s %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Looking at tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "from pprint import pprint\n",
      "client = pymongo.MongoClient()\n",
      "db = client.dsbc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print db.hmm.find().count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweet = db.hmm.find_one({\"favorites\": {\"$gte\": 5}})\n",
      "pprint(tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(tweet['text']).words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Word counts in bitcoin tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_text = \"\"\n",
      "for tweet in db.bitcointweets.find().limit(100):\n",
      "    all_text += tweet[\"text\"] + \"\\n\"\n",
      "    \n",
      "for word, count in sorted(TextBlob(all_text).word_counts.items(), key=get_count, reverse=True)[:15]:\n",
      "    print \"%15s %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Without stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download()\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "\n",
      "all_text = \"\"\n",
      "for tweet in db.bitcointweets.find().limit(100):\n",
      "    all_text += tweet[\"text\"] + \"\\n\"\n",
      "    \n",
      "for word, count in sorted(TextBlob(all_text).word_counts.items(), key=get_count, reverse=True)[:20]:\n",
      "    if word not in stop:\n",
      "        print \"%15s %i\" % (word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Movie Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download()\n",
      "import nltk\n",
      "from textblob import TextBlob\n",
      "from nltk.corpus import movie_reviews\n",
      "\n",
      "fileids = movie_reviews.fileids()[:100]\n",
      "doc_words = [movie_reviews.words(fileid) for fileid in fileids]\n",
      "documents = [' '.join(words) for words in doc_words]\n",
      "print len(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Top bigrams in reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "from collections import defaultdict\n",
      "from operator import itemgetter\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "stop.append(['.', ',', '(', ')', \"'\", '\"'])\n",
      "\n",
      "counter = defaultdict(int)\n",
      "\n",
      "n = 2\n",
      "for doc in documents:\n",
      "    words = TextBlob(doc).words\n",
      "    words = [w for w in words if w not in stop]\n",
      "    bigrams = ngrams(words, n)\n",
      "    print words, n\n",
      "    for gram in bigrams:\n",
      "        counter[gram] += 1\n",
      "            \n",
      "for gram, count in sorted(counter.items(), key = itemgetter(1), reverse=True)[:30]:\n",
      "    phrase = \" \".join(gram)\n",
      "    print '%20s %i' % (phrase, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using Sklearn algorithms with text data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### TF: frequency in this document\n",
      "#### IDF: inverse frequency in the corpus\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
      "doc_vectors = vectorizer.fit_transform(documents)\n",
      "\n",
      "classes = np.array(['pos']*50 + ['neg']*50)\n",
      "\n",
      "\n",
      "model = MultinomialNB().fit(doc_vectors, classes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby_vector = vectorizer.transform([GATSBY_TEXT])\n",
      "model.predict(gatsby_vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(ngrams([1,2,3,4,5], 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")\n",
      "wiki\n",
      "wiki1=TextBlob(documents[0])\n",
      "wiki1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zen = TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zen.sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sentence in zen.sentences:\n",
      "...     print(sentence.sentiment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 1\n",
      "Calculate the tf-idf for each word in the first 100 reviews in the nltk movie reviews corpus. For each document, print the top 10 tf-idf words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "#nltk.download()\n",
      "import nltk\n",
      "from textblob import TextBlob\n",
      "from nltk.corpus import movie_reviews\n",
      "\n",
      "fileids = movie_reviews.fileids()[:100]\n",
      "doc_words = [movie_reviews.words(fileid) for fileid in fileids]\n",
      "documents = [' '.join(words) for words in doc_words]\n",
      "from __future__ import division, unicode_literals\n",
      " \n",
      "def tf(word, blob):\n",
      "    return blob.words.count(word) / len(blob.words)\n",
      " \n",
      "def n_containing(word, bloblist):\n",
      "    return sum(1 for blob in bloblist if word in blob)\n",
      " \n",
      "def idf(word, bloblist):\n",
      "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
      " \n",
      "def tfidf(word, blob, bloblist):\n",
      "    return tf(word, blob) * idf(word, bloblist)\n",
      "\n",
      "bloblist=[] \n",
      "\n",
      "for document in documents:\n",
      "    bloblist.append(document)\n",
      "    \n",
      "for i, blob in enumerate(bloblist):\n",
      "    blob=TextBlob(blob)\n",
      "    #print blob\n",
      "    print(\"Top 10 words in document {}\".format(i + 1))\n",
      "    print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "    for word, score in sorted_words[:10]:\n",
      "        print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 2\n",
      "Calculate the tf-idf for each word in the tweets you uploaded to your mongodb. For each tweet, print the highest tf-idf term. What are potential problems when trying to calculate tf-idf on really short documents like tweets?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem is you won't get an accurate sampling of enough words to calculate an accurate tf-idf, making you calculate a lower tf-idf score for meaningful words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from requests_oauthlib import OAuth1\n",
      "import pprint\n",
      "import pymongo\n",
      "from pync import Notifier\n",
      "from pymongo import MongoClient\n",
      "from pylab import *\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import statsmodels.formula.api as sm\n",
      "import matplotlib.pyplot as pl\n",
      "# Connection to Mongo DB\n",
      "\n",
      "try:\n",
      "    conn=pymongo.MongoClient()\n",
      "    print \"Connected successfully!!!\"\n",
      "except pymongo.errors.ConnectionFailure, e:\n",
      "    print \"Could not connect to MongoDB: %s\" % e \n",
      "conn = pymongo.Connection(\"localhost\")\n",
      "#print type(conn)\n",
      "conn.database_names()\n",
      "db = conn['TwitterAPI']\n",
      "db.collection_names\n",
      "collection = db['BruceLee']\n",
      "#print type(collection)\n",
      "client=MongoClient()\n",
      "db=client.TwitterAPI\n",
      "db.collection_names()\n",
      "BruceLee = client.TwitterAPI.BruceLee\n",
      "c=BruceLee.find({})\n",
      "Tweets = [r['text'] for r in list(c)]\n",
      "bloblist=[] \n",
      "import re\n",
      "\n",
      "for tweet in Tweets:\n",
      "    bloblist.append(tweet)\n",
      "    \n",
      "for i, blob in enumerate(Tweets):\n",
      "    print blob\n",
      "    print blob\n",
      "    #blob=blob.replace(\"t.co/ILgmIONiJN \",\" \")\n",
      "    blob=TextBlob(blob)\n",
      "    #print blob\n",
      "    print(\"Top words in tweet {}\".format(i + 1))\n",
      "    print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "    for word, score in sorted_words[:2]:\n",
      "        print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenges (11-21-14)\n",
      "\n",
      "Use the same code for your previous clustering challenges. Repeat each challenge. However, this time, try (both) Agglomerative Clustering and DBSCAN instead of KMeans. For text clustering, use cosine distance. For non-text clustering, use euclidean distance (but scale the features first!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 1\n",
      "Cluster sentences with K-means. If you have your own Fletcher test data, get sentences out and cluster them. If not, cluster the tweets you gathered during the Twitter API challenge. For each cluster, print out the sentences, try to see how close the sentences are. Try different K values and try to find a K value that makes the most sense (the sentences look like they do form a meaningful cluster).\n",
      "How do you deal with retweets if you're clustering tweets?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from requests_oauthlib import OAuth1\n",
      "import pprint\n",
      "import pymongo\n",
      "from pync import Notifier\n",
      "from pymongo import MongoClient\n",
      "from pylab import *\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import statsmodels.formula.api as sm\n",
      "import matplotlib.pyplot as pl\n",
      "# Connection to Mongo DB\n",
      "\n",
      "try:\n",
      "    conn=pymongo.MongoClient()\n",
      "    print \"Connected successfully!!!\"\n",
      "except pymongo.errors.ConnectionFailure, e:\n",
      "    print \"Could not connect to MongoDB: %s\" % e \n",
      "conn = pymongo.Connection(\"localhost\")\n",
      "#print type(conn)\n",
      "conn.database_names()\n",
      "db = conn['TwitterAPI']\n",
      "db.collection_names\n",
      "collection = db['BruceLee']\n",
      "client=MongoClient()\n",
      "db=client.TwitterAPI\n",
      "db.collection_names()\n",
      "BruceLee = client.TwitterAPI.BruceLee"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "Challenge 1 Cluster sentences with K-means. If you have your own Fletcher test data, get sentences out and cluster them. If not, cluster the tweets you gathered during the Twitter API challenge. For each cluster, print out the sentences, try to see how close the sentences are. Try different K values and try to find a K value that makes the most sense (the sentences look like they do form a meaningful cluster). How do you deal with retweets if you're clustering tweets?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I dealt with retweets by taking them out via finding 'RT' for retweet and only using the set of tweets (only unique tweets)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize\n",
      "import re\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "#sklearn.preprocessing.scale(X)\n",
      "#model = Kmeans.fit(X)\n",
      "#clusters = model.predict(X)\n",
      "#clusters = Kmeans.fit_predict(X)\n",
      "from nltk.tokenize import word_tokenize\n",
      "c=BruceLee.find({})\n",
      "Tweets = [r['text'] for r in list(c)]\n",
      "AllSentences=[]\n",
      "#cleaning out the links\n",
      "for i,tweet in enumerate(Tweets):\n",
      "     tweet = re.sub(r\"amp;*\\s\", \" \", tweet)\n",
      "     #print 'before: ', tweet\n",
      "     tweet = re.sub(\"/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/\",\" \", tweet)\n",
      "     tweet = re.sub(r\"/^<([a-z]+)([^<]+)*(?:>(.*)<\\/\\1>|\\s+\\/>)$/\",\" \", tweet)\n",
      "     tweet = re.sub(r\"http://t.co/....*\", \" \", tweet)\n",
      "     tweet = re.sub(r\"https://t.co/....*\", \"\", tweet)\n",
      "     tweet = re.sub(r\"http:..t.co.[A-Z]*[a-z]*......\", \" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]{1}\\d{4}[a-z]{3}.[a-z]*\\d*[a-z]*\", \" \", tweet)\n",
      "     tweet = re.sub(r\"'[a-z]*\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[A-Z]{1}\\d*[a-z]\\d{3}.[a-z]\\d{3}[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]\\d{4}[a-z]{3}.u\\d*[a-z]{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{3}[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".U\\d*f\\d*.U\\d*f\\d*.U\\d*[a-z]\\d*\\s.U\\d*[a-z]\\d*[a=z]\\s.u\\d*[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d*[a-z]*.u\\d*[a-z]*\\s..[a-z]*\\d*[a-z]*\\s.u\\d*[a-z]*.u[a-z]*\\d*[a-z]*.u\\d*[a-z]*.u\\d*[a-z].[a-z]\\d*[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".n.n\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".U\\d*f\\d*.U\\d*f\\d*.U\\d*f\\d*\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u015ea.u015f.u0131racaks.u0131n.u0131z\",\" \", tweet)\n",
      "     tweet = re.sub(r\".xe9\",\" \",tweet)\n",
      "     tweet = re.sub(r\".u2026\",\" \",tweet)\n",
      "     tweet = re.sub(r\".u0131lmaz\\sG.xf6sterisi\\s.u015ea.u015f.u0131racaks.u0131n.u0131z...\",\" \",tweet)\n",
      "     #tweet = re.sub(r\"Bruce\\sLee\",\" \",tweet)\n",
      "     Tweets[i] = tweet\n",
      "     #print 'after: ', tweet\n",
      "Tweets\n",
      "\n",
      "for tweet in Tweets:\n",
      "     if tweet.find('RT')==-1:\n",
      "          AllSentences.append(tweet)\n",
      "newTweets = list(set(AllSentences))\n",
      "size(newTweets)        \n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
      "doc_vectors = vectorizer.fit_transform(newTweets)\n",
      "\n",
      "max_clusters=30\n",
      "max_clusters=max_clusters+1\n",
      "iall=np.zeros(max_clusters)\n",
      "inertiaall=np.zeros(max_clusters)\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = KMeans(n_clusters=i)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "     print\n",
      "     print\n",
      "     print '------------------', str(i), ' clusters----------------','intertia score: ', str(clustering.inertia_)\n",
      "     print '--------------------------------------------------------'\n",
      "     print\n",
      "     print\n",
      "     \n",
      "     \n",
      "     for mm in range(0,i):\n",
      "            print\n",
      "            print\n",
      "            print '********************************************************'\n",
      "            print '*******cluster: ', mm+1\n",
      "            print '********************************************************'\n",
      "            print\n",
      "            print\n",
      "            for ll, kk in enumerate(clusters):\n",
      "                if kk==mm:\n",
      "                    print AllSentences[ll]\n",
      "                    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 2\n",
      "Draw the inertia curve over different k values. (Sklearn KMeans class has an inertia_ attribute)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Challenge 2\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'ro')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('Kmeans inertia value by cluster number')\n",
      "plt.show()\n",
      "print type(doc_vectors), type(newTweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 3\n",
      "\n",
      "Let's name the clusters 1\n",
      "For each cluster, find the sentence closest to the centroid of the (You can learn sklearn.metrics.pairwise_distances4 or scipy.spatial.distance3 (check pdist, cdist, and euclidean distance) to find distances to the centroid. KMeans has a cluster_centroids_ attribute. This sentence (closest to centroid) is now the name of the cluster. For each cluster, print the representative sentence, and print 'N people expressed a similar statement', or something like that relevant to your dataset. (This is very close to what amazon used to do in the reviews section up to a year ago.)\n",
      "\n",
      "Find the biggest 3 clusters, and print their representative sentences (This is close to what amazon is doing now in the reviews section, except they choose the sentence from the most helpful review instead of closest to center)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import pairwise_distances\n",
      "i=3\n",
      "clustering = KMeans(n_clusters=i,n_init=100)\n",
      "model=clustering.fit(doc_vectors)\n",
      "clusters=model.predict(doc_vectors)\n",
      "centroids=model.cluster_centers_\n",
      "doc_vectors_full=doc_vectors.todense()\n",
      "hh=model.labels_\n",
      "cluster1=[]\n",
      "cluster0=[]\n",
      "cluster2=[]\n",
      "Tweet0=[]\n",
      "Tweet1=[]\n",
      "Tweet2=[]\n",
      "for i in range(0,doc_vectors_full.shape[0]):\n",
      "    if hh[i]==0:\n",
      "        cluster0.append(doc_vectors_full[i]) \n",
      "        Tweet0.append(newTweets[i])\n",
      "    if hh[i]==1:\n",
      "        cluster1.append(doc_vectors_full[i])\n",
      "        Tweet1.append(newTweets[i])\n",
      "    if hh[i]==2:\n",
      "        cluster2.append(doc_vectors_full[i])\n",
      "        Tweet2.append(newTweets[i])\n",
      "\n",
      "cluster0 = np.squeeze(np.asarray(cluster0))\n",
      "cluster1 = np.squeeze(np.asarray(cluster1))\n",
      "cluster2 = np.squeeze(np.asarray(cluster2))\n",
      "pd0=pairwise_distances(cluster0,centroids[0])\n",
      "pd1=pairwise_distances(cluster1,centroids[1])\n",
      "pd2=pairwise_distances(cluster2,centroids[2])\n",
      "pd0_min = min(pd0)\n",
      "pd1_min = min(pd1)\n",
      "pd2_min = min(pd2)\n",
      "\n",
      "for i in range(0,len(pd0)):\n",
      "    if pd0[i]==pd0_min:\n",
      "        print 'representative tweet cluster 1: ', Tweet0[i]\n",
      "        break\n",
      "for i in range(0,len(pd1)):\n",
      "    if pd1[i]==pd1_min:\n",
      "        print 'representative tweet cluster 2: ', Tweet1[i]\n",
      "        break\n",
      "for i in range(0,len(pd2)):\n",
      "    if pd2[i]==pd2_min:\n",
      "        print 'representative tweet cluster 3: ', Tweet2[i]\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 4\n",
      "Let's name the clusters 2 Calculate the tf-idf of each word in each cluster (think of all sentences of a cluster together as a document). Represent each cluster with the top 1, or top 2 or... to 5 tf-idf words. For each cluster, print the name (keywords) of the cluster, and \"N statements\" in the cluster (N is the size of the cluster)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bloblist=[]\n",
      "\n",
      "for tweet in Tweet0:\n",
      "    bloblist.append(tweet)\n",
      "\n",
      "\n",
      "for i, blob in enumerate(Tweet0):\n",
      "    blob=TextBlob(blob)\n",
      "    #print blob\n",
      "    #print(\"Top 3 words in tweet {}\".format(i + 1))\n",
      "    #print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 1-------------------\"\n",
      "print str(len(Tweet0)),\" statements\"\n",
      "print \"---------------top 5 words ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    \n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))\n",
      "    \n",
      "bloblist=[]   \n",
      "for tweet in Tweet1:\n",
      "    bloblist.append(tweet)\n",
      "\n",
      "\n",
      "for i, blob in enumerate(Tweet1):\n",
      "    blob=TextBlob(blob)\n",
      "    #print blob\n",
      "    #print(\"Top 3 words in tweet {}\".format(i + 1))\n",
      "    #print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 2-------------------\"\n",
      "print str(len(Tweet1)),\" statements\"\n",
      "print \"---------------top 5 words ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    \n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))\n",
      "\n",
      "bloblist=[]\n",
      "for tweet in Tweet2:\n",
      "    bloblist.append(tweet)\n",
      "\n",
      "\n",
      "for i, blob in enumerate(Tweet2):\n",
      "    blob=TextBlob(blob)\n",
      "    #print blob\n",
      "    #print(\"Top 3 words in tweet {}\".format(i + 1))\n",
      "    #print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 3-------------------\"\n",
      "print str(len(Tweet2)),\" statements\"\n",
      "print \"---------------top 5 words ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    \n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 5\n",
      "\n",
      "Let's name the clusters 3\n",
      "Same as the previous challenge, but this time, calculate tf-idf only for nouns (NN tag) and build keyword(s) with nouns. (This is close to what amazon switched to last year, before settling into the current design). (They would show five nouns, you would click on one and it would show sentences - linked to the reviews- that were related to that noun.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bloblist=[] #needs to only be all the nouns\n",
      "nounTweet0=[]\n",
      "for i,tweet in enumerate(Tweet0):\n",
      "    words=[]\n",
      "    taggedWords=[]\n",
      "    words=word_tokenize(tweet)\n",
      "    taggedWords=pos_tag(words)\n",
      "    for j,word in enumerate(taggedWords):\n",
      "         if taggedWords[j][1]=='NN':\n",
      "             nounTweet0.append(taggedWords[j][0])\n",
      "    \n",
      "bloblist=str(nounTweet0)\n",
      "\n",
      "for i, blob in enumerate(bloblist):\n",
      "    blob=TextBlob(bloblist)\n",
      "    #print blob\n",
      "    #print(\"Top 3 words in tweet {}\".format(i + 1))\n",
      "    #print(\"------------------------------\")\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 1-------------------\"\n",
      "print \"---------------top 5 keywords ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))\n",
      "\n",
      "    \n",
      "    \n",
      "bloblist=[] #needs to only be all the nouns\n",
      "nounTweet1=[]\n",
      "for i,tweet in enumerate(Tweet1):\n",
      "    words=[]\n",
      "    taggedWords=[]\n",
      "    words=word_tokenize(tweet)\n",
      "    taggedWords=pos_tag(words)\n",
      "    for j,word in enumerate(taggedWords):\n",
      "         if taggedWords[j][1]=='NN':\n",
      "             nounTweet1.append(taggedWords[j][0])\n",
      "    \n",
      "bloblist=str(nounTweet1)\n",
      "\n",
      "for i, blob in enumerate(bloblist):\n",
      "    blob=TextBlob(bloblist)\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "    #print scores\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 2-------------------\"\n",
      "print \"---------------top keywords ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))\n",
      "    \n",
      "    \n",
      "    \n",
      "bloblist=[] #needs to only be all the nouns\n",
      "nounTweet2=[]\n",
      "for i,tweet in enumerate(Tweet2):\n",
      "    words=[]\n",
      "    taggedWords=[]\n",
      "    words=word_tokenize(tweet)\n",
      "    taggedWords=pos_tag(words)\n",
      "    for j,word in enumerate(taggedWords):\n",
      "         if taggedWords[j][1]=='NN':\n",
      "             nounTweet2.append(taggedWords[j][0])\n",
      "    \n",
      "bloblist=str(nounTweet2)\n",
      "\n",
      "for i, blob in enumerate(bloblist):\n",
      "    blob=TextBlob(bloblist)\n",
      "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
      "sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "print \"-----------------cluster 3-------------------\"\n",
      "print \"---------------top keywords ------------------\"\n",
      "for word, score in sorted_words[:5]:\n",
      "    print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 10)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 6\n",
      "\n",
      "Cluster the same data with MiniBatchKMeans1. MiniBatchKMeans is a fast way to apply K-means to large data without much loss -- The results are very similar. Instead of using EVERY single point to find the new place of the centroid, MiniBatch just randomly samples a small number (like 100) in the cluster to calculate the new center. Since this is usually very close to the actual center, the algorithm gets there much faster. Try it and compare the results. Example on two-feature data8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import MiniBatchKMeans\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
      "doc_vectors = vectorizer.fit_transform(newTweets)\n",
      "\n",
      "max_clusters=30\n",
      "max_clusters=max_clusters+1\n",
      "iall=np.zeros(max_clusters)\n",
      "inertiaall=np.zeros(max_clusters)\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = MiniBatchKMeans(n_clusters=i)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "     print\n",
      "     print\n",
      "     print '------------------', str(i), ' clusters----------------','intertia score: ', str(clustering.inertia_)\n",
      "     print '--------------------------------------------------------'\n",
      "     print\n",
      "     print\n",
      "     \n",
      "     \n",
      "     for mm in range(0,i):\n",
      "            print\n",
      "            print\n",
      "            print '********************************************************'\n",
      "            print '*******cluster: ', mm+1\n",
      "            print '********************************************************'\n",
      "            print\n",
      "            print\n",
      "            for ll, kk in enumerate(clusters):\n",
      "                if kk==mm:\n",
      "                    print AllSentences[ll]\n",
      "                    \n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'ro')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('Kmeans inertia value by cluster number')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 7\n",
      "\n",
      "Switch the init parameter to \"random\" (instead of the default kmeans++) and plot the inertia curve for each of the n_init values for K-Means: 1, 2, 3, 10 (n_init is the number of different runs to try with different random initializations)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_clusters=30\n",
      "max_clusters=max_clusters+1\n",
      "iall=np.zeros(max_clusters)\n",
      "inertiaall=np.zeros(max_clusters)\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = MiniBatchKMeans(init='random',n_clusters=i,n_init=1)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "                    \n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'ro')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('MiniBatchKmeans: random init, n_init=1')\n",
      "plt.show()\n",
      "\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = MiniBatchKMeans(init='random',n_clusters=i,n_init=2)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "                    \n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'bo')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('MiniBatchKmeans: random init, n_init=2')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = MiniBatchKMeans(init='random',n_clusters=i,n_init=3)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "                    \n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'go')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('MiniBatchKmeans: random init, n_init=3')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = MiniBatchKMeans(init='random',n_clusters=i,n_init=10)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "                    \n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'ko')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('MiniBatchKmeans: random init, n_init=10')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 8\n",
      "\n",
      "Download this dataset on the purchase stats from clients of a wholesale distributor10. Cluster the clients based on their annual spending features (fresh, milk, grocery, frozen, detergents_paper, delicatessen). Remember to scale the features before clustering. After finding a reasonable amount of clusters, for EACH cluster, plot the histogram for every single feature: FRESH, MILK, GROCERY, FROZEN, DETERGENTS_PAPER, DELICATESSEN, CHANNEL, REGION. Is there a natural way to characterize each cluster? How would you describe each cluster to the wholesale distributor if you were working for them?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A natural way to cluster them would be to use see which people spent the money on the features in a particular order (e.g. person A spends most on frozen, then milk, then grocery, etc.)\n",
      "I'd tell the client that each cluster represents a certain pattern of spending that is common among the group as well as the channel and region that this group is found"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline    \n",
      "import matplotlib.pyplot as pl\n",
      "import pandas as pd\n",
      "from sklearn import preprocessing\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "df = pd.read_csv('wholesale.csv')\n",
      "df_scale = pd.read_csv('wholesale.csv')\n",
      "df_scale[['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']]=preprocessing.scale(df_scale[['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']]) \n",
      "\n",
      "df_scale.to_csv('Scaled.csv')\n",
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "data=[]\n",
      "data = numpy.loadtxt('Scaled2.csv', dtype=float, delimiter=',')\n",
      "\n",
      "\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
      "doc_vectors = (data)\n",
      "\n",
      "\n",
      "max_clusters=50\n",
      "max_clusters=max_clusters+1\n",
      "iall=np.zeros(max_clusters)\n",
      "inertiaall=np.zeros(max_clusters)\n",
      "for i in range(2,max_clusters):\n",
      "     clustering = KMeans(i)\n",
      "     model=clustering.fit(doc_vectors)\n",
      "     clusters=model.predict(doc_vectors)\n",
      "     iall[i-2]=i\n",
      "     inertiaall[i-2]=clustering.inertia_\n",
      "     print '------------------', str(i), ' clusters----------------','intertia score: ', str(clustering.inertia_)\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as pl\n",
      "pl.plot(iall, inertiaall, 'ro')\n",
      "pl.xlabel('clusters')\n",
      "pl.ylabel('inertia value')\n",
      "pl.title('Kmeans inertia value by cluster number')\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "30 appears to be a good cluster number that minimizes inertia"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_vectors = (data)\n",
      "i=30\n",
      "import pprint\n",
      "import pymongo\n",
      "from pync import Notifier\n",
      "from pymongo import MongoClient\n",
      "from pylab import *\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import statsmodels.formula.api as sm\n",
      "import matplotlib.pyplot as pl\n",
      "\n",
      "clustering = KMeans(i)\n",
      "model=clustering.fit(doc_vectors)\n",
      "clusters=model.predict(doc_vectors)\n",
      "\n",
      "hh=model.labels_\n",
      "\n",
      "#create cluster var's\n",
      "for x in range(1,i+1):\n",
      "\texec(\"cluster\"+str(x)+\"=\"+\"[]\")\n",
      "#fill in cluster var's\n",
      "for x in range(0,hh.shape[0]):\n",
      "    g=hh[x]\n",
      "    exec(\"cluster\"+str(g+1)+\".append(doc_vectors[x])\")\n",
      "for x in range(1,31):    \n",
      "    exec(\"df\"+str(x)+\"=pd.DataFrame(cluster\"+str(x)+\", columns=['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen'])\")\n",
      "for x in range(1,31):\n",
      "    #scaled histos\n",
      "    \n",
      "    exec(\"df_scale=df\"+str(x))\n",
      "    exec(\"len1Test=len(cluster\"+str(x)+\")\")\n",
      "    if len1Test!=1:\n",
      "        #print '-------cluster',x,'-------'\n",
      "        pl.figure()\n",
      "        pl.subplot(3, 2, 1)   \n",
      "        df_scale['Grocery'].hist()\n",
      "        pl.title('grocery--scaled'+'CLUSTER'+str(x)+'***')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "\n",
      "        pl.subplot(3, 2, 2)\n",
      "        df_scale['Fresh'].hist()\n",
      "        pl.title('fresh--scaled')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "\n",
      "        pl.subplot(3, 2, 3)\n",
      "        df_scale['Milk'].hist()\n",
      "        pl.title('milk--scaled')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "        \n",
      "        pl.subplot(3, 2, 6)\n",
      "        df_scale['Frozen'].hist()\n",
      "        pl.title('Frozen--scaled')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "\n",
      "        pl.subplot(3, 2, 4)\n",
      "        df_scale['Detergents_Paper'].hist()\n",
      "        pl.title('detergents paper--scaled')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "\n",
      "        pl.subplot(3, 2, 5)\n",
      "        df_scale['Delicassen'].hist()\n",
      "        pl.title('delicassen--scaled')\n",
      "        pl.ylabel('freq')\n",
      "        pl.xlabel('$$$')\n",
      "        \n",
      "        savefig('cluster'+str(x)+'.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenges\n",
      "\n",
      "Use the same code for your previous clustering challenges. Repeat each challenge (except the inertia curves, since only the KMeans implementation gives a quick way of calculating that.)\n",
      "However, this time, try (both) Agglomerative Clustering and DBSCAN instead of KMeans. For text clustering, use cosine distance. For non-text clustering, use euclidean distance (but scale the features first!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Challenge 1B Cluster sentences with Agglomerative Clustering and DBSCAN. If you have your own Fletcher test data, \n",
      "get sentences out and cluster them. If not, cluster the tweets you gathered during the Twitter API challenge. \n",
      "For each cluster, print out the sentences, try to see how close the sentences are. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import AgglomerativeClustering\n",
      "import requests\n",
      "from requests_oauthlib import OAuth1\n",
      "import pprint\n",
      "import pymongo\n",
      "from pync import Notifier\n",
      "from pymongo import MongoClient\n",
      "from pylab import *\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import statsmodels.formula.api as sm\n",
      "import matplotlib.pyplot as pl\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "# Connection to Mongo DB\n",
      "\n",
      "try:\n",
      "    conn=pymongo.MongoClient()\n",
      "    print \"Connected successfully!!!\"\n",
      "except pymongo.errors.ConnectionFailure, e:\n",
      "    print \"Could not connect to MongoDB: %s\" % e \n",
      "conn = pymongo.Connection(\"localhost\")\n",
      "#print type(conn)\n",
      "conn.database_names()\n",
      "db = conn['TwitterAPI']\n",
      "db.collection_names\n",
      "collection = db['BruceLee']\n",
      "client=MongoClient()\n",
      "db=client.TwitterAPI\n",
      "db.collection_names()\n",
      "BruceLee = client.TwitterAPI.BruceLee\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "import re\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "from nltk.tokenize import word_tokenize\n",
      "c=BruceLee.find({})\n",
      "Tweets = [r['text'] for r in list(c)]\n",
      "AllSentences=[]\n",
      "#cleaning out the links\n",
      "for i,tweet in enumerate(Tweets):\n",
      "     tweet = re.sub(r\"amp;*\\s\", \" \", tweet)\n",
      "     tweet = re.sub(\"/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/\",\" \", tweet)\n",
      "     tweet = re.sub(r\"/^<([a-z]+)([^<]+)*(?:>(.*)<\\/\\1>|\\s+\\/>)$/\",\" \", tweet)\n",
      "     tweet = re.sub(r\"http://t.co/....*\", \" \", tweet)\n",
      "     tweet = re.sub(r\"https://t.co/....*\", \"\", tweet)\n",
      "     tweet = re.sub(r\"http:..t.co.[A-Z]*[a-z]*......\", \" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]{1}\\d{4}[a-z]{3}.[a-z]*\\d*[a-z]*\", \" \", tweet)\n",
      "     tweet = re.sub(r\"'[a-z]*\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[A-Z]{1}\\d*[a-z]\\d{3}.[a-z]\\d{3}[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".[a-z]\\d{4}[a-z]{3}.u\\d*[a-z]{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{3}[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".U\\d*f\\d*.U\\d*f\\d*.U\\d*[a-z]\\d*\\s.U\\d*[a-z]\\d*[a=z]\\s.u\\d*[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d*[a-z]*.u\\d*[a-z]*\\s..[a-z]*\\d*[a-z]*\\s.u\\d*[a-z]*.u[a-z]*\\d*[a-z]*.u\\d*[a-z]*.u\\d*[a-z].[a-z]\\d*[a-z]\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".n.n\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u\\d{4}\",\" \", tweet)\n",
      "     tweet = re.sub(r\".U\\d*f\\d*.U\\d*f\\d*.U\\d*f\\d*\",\" \", tweet)\n",
      "     tweet = re.sub(r\".u015ea.u015f.u0131racaks.u0131n.u0131z\",\" \", tweet)\n",
      "     tweet = re.sub(r\".xe9\",\" \",tweet)\n",
      "     tweet = re.sub(r\".u2026\",\" \",tweet)\n",
      "     tweet = re.sub(r\".u0131lmaz\\sG.xf6sterisi\\s.u015ea.u015f.u0131racaks.u0131n.u0131z...\",\" \",tweet)\n",
      "     Tweets[i] = tweet\n",
      "for tweet in Tweets:\n",
      "     if tweet.find('RT')==-1:\n",
      "          AllSentences.append(tweet)\n",
      "newTweets = list(set(AllSentences))     \n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
      "doc_vectors = vectorizer.fit_transform(newTweets)\n",
      "#Agglomerative Clustering\n",
      "clustering = AgglomerativeClustering(linkage='average',affinity='cosine',n_clusters=10)\n",
      "doc_vectors=doc_vectors.todense()\n",
      "model=clustering.fit(doc_vectors)\n",
      "clusters=model.fit_predict(doc_vectors)\n",
      "clusterSet=set(clusters) \n",
      "print\n",
      "print\n",
      "print 'Agglomerative Clustering until 10 clusters!'\n",
      "for i in range (0,len(clusterSet)):\n",
      "    print '********************************************************'\n",
      "    print '*******cluster: ', i+1\n",
      "    print '********************************************************'\n",
      "    for k in range(0,len(newTweets)):\n",
      "        if clusters[k]==i:\n",
      "            print newTweets[k]\n",
      "            \n",
      "#DBSCAN\n",
      "from scipy.spatial import distance\n",
      "from sklearn.cluster import DBSCAN\n",
      "from sklearn import metrics\n",
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "\n",
      "###############################################################################DBSCAN\n",
      "clustering = DBSCAN(algorithm='brute',metric='cosine')\n",
      "model=clustering.fit(doc_vectors)\n",
      "clusters=model.fit_predict(doc_vectors)\n",
      "model.labels_\n",
      "clusterSet=set(model.labels_) \n",
      "\n",
      "print\n",
      "print\n",
      "print 'DBSCAN'\n",
      "for i in range (0,len(clusterSet)):\n",
      "    print '********************************************************'\n",
      "    print '*******cluster: ', i+1\n",
      "    print '********************************************************'\n",
      "    for k in range(0,len(newTweets)):\n",
      "        if clusters[k]==model.labels_[i]:\n",
      "            print newTweets[k]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "DBSCAN\n",
        "********************************************************\n",
        "*******cluster:  1\n",
        "********************************************************\n",
        ".@SlalomChicago shares wisdom from Bruce Lee  teachings to absorb what is useful and discard what is not #BESTDESIGNFC\n",
        "#Bruce Lee Its \"An Orphan  Tragedy\" if \"The Kid\" NOT saved from the \"Game of Death\" @ ACC! \n",
        "- Bruce Lee  \n",
        ".Bruce Lee   \u0131lmaz G\u00f6sterisi \u015ea\u015f\u0131racaks\u0131n\u0131z...  \n",
        "@Tragic_Kev simplicity is key to brilliance- Bruce Lee\n",
        "A quick temper will make a fool of you soon enough ~ Bruce Lee\n",
        "What Bruce Lee can teach you about design :)  \n",
        "\u201c@FastCompany: What Bruce Lee can teach you about design  \n",
        "What Bruce Lee Can Teach You About Design  \n",
        "If you love life, don  waste time, for time is what life is made up of - Bruce Lee\n",
        "Bruce Lee \u2014 \"Don  fear failure. \u2014 Not failure, but low aim, is the crime. In great attempts it is glorious even to fail.\"\n",
        "#Mspirit\n",
        "\"@TheUnusualFact: Bruce Lee was so fast that his films often had to be slowed down in order for viewers to see his moves.\"\n",
        "Photoset: \"I just want Bruce Lee to hold me as long as he can.\" I\u2019m dying.  \n",
        "A goal is not always meant to be reached, it often serves simply as something to aim at. ~ Bruce Lee\n",
        "\"Knowing is not enough, we must apply. Willing is not enough, we must do.\" - Bruce Lee\n",
        "If you spend too much time thinking about a thing, you  never get it done.-Bruce Lee\n",
        "Retroceder ca, rendirse jam\u00e1s (1985) Pel\u00edculas de serie H porque la B se quedo muy atr\u00e1s. Bruce Lee ghost teacher!!!\n",
        "Bruce Lee was so fast, that they actually had to slow a film down so you could see his moves.\n",
        "I don  care if hes Mohammed-Im ard- Bruce Lee you cant change fighters!\n",
        "EvanMalakay - \"Bruce Lee\" HD) \n",
        "\u201c@AlyssaaGarciaa: Bruce Lee was sooooo hot \ud83d\ude29\u201d his son was too!!! \ud83d\udc95\ud83d\udc95\n",
        "Can  find my Bruce Lee shirt = Day ruined\n",
        "\u00abBe water, my friend.\u00bb\n",
        "\n",
        "Bruce Lee\n",
        "Natafta movie za Bruce Lee sana, i akonayo huku?\n",
        "Defeat is not defeat unless accepted as a reality-in your own mind. - Bruce Lee\n",
        "********************************************************\n",
        "*******cluster:  2\n",
        "********************************************************\n",
        ".@SlalomChicago shares wisdom from Bruce Lee  teachings to absorb what is useful and discard what is not #BESTDESIGNFC\n",
        "#Bruce Lee Its \"An Orphan  Tragedy\" if \"The Kid\" NOT saved from the \"Game of Death\" @ ACC! \n",
        "- Bruce Lee  \n",
        ".Bruce Lee   \u0131lmaz G\u00f6sterisi \u015ea\u015f\u0131racaks\u0131n\u0131z...  \n",
        "@Tragic_Kev simplicity is key to brilliance- Bruce Lee\n",
        "A quick temper will make a fool of you soon enough ~ Bruce Lee\n",
        "What Bruce Lee can teach you about design :)  \n",
        "\u201c@FastCompany: What Bruce Lee can teach you about design  \n",
        "What Bruce Lee Can Teach You About Design  \n",
        "If you love life, don  waste time, for time is what life is made up of - Bruce Lee\n",
        "Bruce Lee \u2014 \"Don  fear failure. \u2014 Not failure, but low aim, is the crime. In great attempts it is glorious even to fail.\"\n",
        "#Mspirit\n",
        "\"@TheUnusualFact: Bruce Lee was so fast that his films often had to be slowed down in order for viewers to see his moves.\"\n",
        "Photoset: \"I just want Bruce Lee to hold me as long as he can.\" I\u2019m dying.  \n",
        "A goal is not always meant to be reached, it often serves simply as something to aim at. ~ Bruce Lee\n",
        "\"Knowing is not enough, we must apply. Willing is not enough, we must do.\" - Bruce Lee\n",
        "If you spend too much time thinking about a thing, you  never get it done.-Bruce Lee\n",
        "Retroceder ca, rendirse jam\u00e1s (1985) Pel\u00edculas de serie H porque la B se quedo muy atr\u00e1s. Bruce Lee ghost teacher!!!\n",
        "Bruce Lee was so fast, that they actually had to slow a film down so you could see his moves.\n",
        "I don  care if hes Mohammed-Im ard- Bruce Lee you cant change fighters!\n",
        "EvanMalakay - \"Bruce Lee\" HD) \n",
        "\u201c@AlyssaaGarciaa: Bruce Lee was sooooo hot \ud83d\ude29\u201d his son was too!!! \ud83d\udc95\ud83d\udc95\n",
        "Can  find my Bruce Lee shirt = Day ruined\n",
        "\u00abBe water, my friend.\u00bb\n",
        "\n",
        "Bruce Lee\n",
        "Natafta movie za Bruce Lee sana, i akonayo huku?\n",
        "Defeat is not defeat unless accepted as a reality-in your own mind. - Bruce Lee\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}